1. **什么是AI大模型？**  
   指参数量巨大（通常超过10亿）、通过海量数据训练的深度学习模型（如GPT、BERT、LLaMA等），具备强大的泛化能力和多任务处理能力。

2. **大模型与小模型的核心区别？**  
   规模（参数量/数据量）、泛化能力、多任务支持、训练成本及推理资源需求。

3. **大模型参数规模的意义？**  
   参数越多模型表达能力越强，但需平衡计算效率与性能收益（并非绝对正相关）。

4. **Transformer架构是什么？**  
   2017年提出的核心架构，依赖自注意力机制处理序列数据，成为大模型（如GPT/BERT）的基础。

5. **什么是自注意力机制？**  
   通过计算序列内部元素关联权重，动态分配关注度，解决长距离依赖问题。

6. **预训练的作用？**  
   在无标注数据上学习通用语言/视觉表征，为下游任务提供知识基础。

7. **微调（Fine-tuning）是什么？**  
   在预训练模型基础上，用特定领域数据调整参数以适应具体任务（如情感分析）。

8. **提示工程（Prompt Engineering）？**  
   设计输入文本引导模型生成期望输出，减少微调需求（如："请用一句话总结下文："）。

9. **RLHF（人类反馈强化学习）？**  
   通过人类评分优化模型输出，使结果更符合人类价值观（如ChatGPT的训练关键步骤）。

10. **参数高效微调（PEFT）有哪些方法？**  
    LoRA（低秩适配）、Adapter（插入小模块）、Prefix-tuning（前缀优化）等，仅微调少量参数。

11. **大模型推理为何需要GPU？**  
    矩阵运算高度并行，GPU的数千核心比CPU更高效。

12. **显存不足如何解决？**  
    量化（FP16->INT8）、模型切分（ZeRO）、梯度检查点、使用云服务。

13. **什么是量化（Quantization）？**  
    降低模型权重/激活值的数值精度（如32位浮点→8位整数），减少显存占用和延迟。

14. **大模型如何部署到移动端？**  
    蒸馏（训练小模型）、量化、剪枝、专用框架（Core ML/TensorFlow Lite）。

15. **API调用 vs 本地部署的优劣？**  
    API：低成本易用但依赖网络；本地部署：数据安全可控但需硬件资源。

16. **典型文本生成应用？**  
    写作助手、代码生成、对话机器人、摘要生成。

17. **大模型如何用于搜索？**  
    理解查询语义、生成答案（如New Bing）、优化排序结果。

18. **代码大模型示例？**  
    GitHub Copilot（基于Codex）、AlphaCode、StarCoder。

19. **多模态大模型是什么？**  
    同时处理文本/图像/音频（如GPT-4V、DALL·E、Sora）。

20. **垂直行业如何应用大模型？**  
    金融研报生成、医疗问答、法律合同审查（需领域数据微调）。

21. **大模型存在哪些偏见？**  
    训练数据中的社会偏见（性别/种族）可能被放大。

22. **如何缓解幻觉（Hallucination）？**  
    检索增强（RAG）、事实一致性校验、限制生成范围。

23. **数据隐私如何保护？**  
    差分隐私训练、联邦学习、数据脱敏、本地化部署。

24. **模型滥用风险？**  
    生成虚假信息、钓鱼邮件、恶意代码——需内容过滤和监管。

25. **AI对齐（AI Alignment）问题？**  
    确保模型目标与人类价值观一致（如不生成有害内容）。

26. **主流开源大模型？**  
    LLaMA（Meta）、BLOOM（BigScience）、Falcon（TII）、Qwen（阿里）、Baichuan（百川）。

27. **Hugging Face平台的作用？**  
    提供模型仓库、数据集、训练工具（Transformers库）的一站式平台。

28. **LangChain是什么？**  
    开源框架，简化大模型应用开发（连接数据源/工具链）。

29. **LlamaIndex的核心功能？**  
    为私有数据构建索引，实现高效检索增强（RAG）。

30. **大模型开源协议注意事项？**  
    部分模型限制商用（如LLaMA 2可商用但需申请），需仔细阅读条款。

31. **降低训练成本的方法？**  
    混合精度训练、梯度累积、模型并行、云平台竞价实例。

32. **vLLM是什么？**  
    高吞吐推理框架，通过PagedAttention优化显存管理。

33. **FlashAttention如何加速训练？**  
    优化注意力计算顺序，减少GPU显存访问次数。

34. **模型蒸馏（Distillation）原理？**  
    用小模型学习大模型的输出分布，保留性能的同时减少规模。

35. **MoE（Mixture of Experts）架构？**  
    仅激活部分专家网络处理输入，提升模型容量且控制计算量（如Grok-1）。

36. **PyTorch vs TensorFlow？**  
    PyTorch研究更流行（动态图），TensorFlow生产部署更成熟（静态图）。

37. **DeepSpeed的作用？**  
    Microsoft开发的训练优化库，支持ZeRO并行/混合精度。

38. **LLaMA.cpp的意义？**  
    纯C++实现的大模型推理，支持CPU运行及轻量化部署。

39. **GGUF文件格式？**  
    替代GGML的量化模型格式，支持元数据嵌入。

40. **TensorRT-LLM功能？**  
    NVIDIA推出的推理优化引擎，提升GPU利用率。

41. **主流中文大模型？**  
    通义千问（阿里）、文心一言（百度）、ChatGLM（清华）、Kimi（月之暗面）。

42. **中英双语模型的优势？**  
    更好处理混合语言场景（如跨境电商、学术翻译）。

43. **中文微调常用数据集？**  
    COIG、Firefly、Alpaca-CN等人工构造指令集。

44. **为什么需要专门的中文tokenizer？**  
    中文字符需分词，优化编码效率（如WordPiece vs BPE）。

45. **中文大模型评测基准？**  
    C-Eval、GaokaoBench、LongBench中文版。

46. **多模态是否是必然方向？**  
    是，文本/图像/视频/音频融合是通用AI的关键路径。

47. **MoE会成为主流架构吗？**  
    在处理效率与模型规模平衡上有优势（如Mixtral 8x7B已证明）。

48. **长文本处理的进展？**  
    Context Window扩展（Claude 200K）、检索增强、结构化记忆。

49. **AI Agent是什么？**  
    能自主规划、使用工具、执行复杂任务的大模型系统（如AutoGPT）。

50. **模型小型化趋势？**  
    终端设备需求推动蒸馏、量化、稀疏化技术发展。